{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PDRPC_simulation_study.ipynb","provenance":[],"authorship_tag":"ABX9TyNk9pRLIrctuM34HS+5vpc/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lb1nH7uCKTVR"},"source":["# Install and load packages"]},{"cell_type":"code","metadata":{"id":"vUyO1ny_KLuN"},"source":["!pip install factor_analyzer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gQPo6i_-KX4S"},"source":["!pip install scikit-learn==0.24.2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vuq9nJMjKX7D"},"source":["!pip install nolds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8XzJWIL6KX-f"},"source":["!git clone https://github.com/josemiotto/pylevy\n","\n","# navigate to atalaia directory\n","%cd pylevy\n","\n","# get modifications made on the repo\n","!git pull origin master\n","\n","# install packages requirements\n","#!pip install -r requirements.txt\n","\n","# install package\n","!python setup.py install"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-cW_wzYKp2H"},"source":["%cd /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pf5LnRmcKvVp"},"source":["# Import packages\n","import pandas as pd\n","import numpy as np\n","import scipy.stats as st\n","from scipy.stats import norm, gumbel_l\n","import sklearn as skl\n","from sklearn.linear_model import LinearRegression \n","from sklearn.cluster import KMeans, AgglomerativeClustering\n","from sklearn.preprocessing import StandardScaler\n","from sklearn import metrics\n","from sklearn.metrics.cluster import adjusted_rand_score\n","from sklearn.decomposition import PCA, FactorAnalysis\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","import random\n","import seaborn as sb\n","import factor_analyzer as fa\n","from nolds import hurst_rs\n","import levy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JCjCfMV6K3KV"},"source":["# Preparation"]},{"cell_type":"code","metadata":{"id":"0QunOOlSK6ku"},"source":["NumSim = 200 #Number of Simulations\n","t = 250 #time, i.e. number of observations\n","n = 600 #number of time series\n","\n","features = ['std', 'skew', 'kurt', 'VaR0.05', 'ES', 'beta', 'VaR0.95', 'EU', 'autocor', 'Hurst', 'stab_a', 'stab_g']\n","num_feat = len(features) #number of features \n","num_clust = 3 #number of clusters: in simulation we know the true value. With real world data we have to try different ones.\n","\n","\n","name_models = ['pca2_km', 'pca3_km', 'pca2_ac', 'pca3_ac',\n","               'fa2_km', 'fa3_km', 'fa2v_km', 'fa3v_km',\n","               'fa2_ac', 'fa3_ac', 'fa2v_ac', 'fa3v_ac']\n","NumModels = len(name_models) #number of models"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ueogTX8hKvXY"},"source":["y_true = np.zeros(n, dtype=int)\n","y_true[int(n/3):int(2*n/3)]= 1\n","y_true[int(2*n/3):] = 2\n","\n","#To store the features\n","Data_save = np.zeros((NumSim, n+1, num_feat))\n","\n","#Adj_Rand: to save the Adj_Rand_Scores\n","Adj_Rand = pd.DataFrame(np.zeros((NumSim, NumModels)), columns = name_models)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mS93aNR5LaMU"},"source":["def km(model, X, y):\n","  kmeans = model.fit(X)\n","  yhat = kmeans.predict(X)\n","  a = adjusted_rand_score(y, yhat)\n","  return a\n","\n","def agc(X, y):\n","  yhat = AgglomerativeClustering(n_clusters=num_clust).fit_predict(X)\n","  a = adjusted_rand_score(y, yhat)\n","  return a"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XP5WX86sL-EP"},"source":["# Simulate data and calculate features"]},{"cell_type":"code","metadata":{"id":"EYTqMuNoL9MH"},"source":["for i in range(NumSim):\n","  TSData = np.zeros((t, n+1)) #n+1 because we need market proxy TS\n","  for j in range(int(n/3)):\n","    TSData[:,j] = norm.rvs(loc=0, scale=0.7, size=t) #loc=mean, scale=standard deviation\n","    TSData[:,j+int(n/3)] = norm.rvs(loc=0, scale=0.2, size=t)\n","    TSData[:,j+2*int(n/3)] = gumbel_l.rvs(size=t)\n","    TSData[:, n] = np.mean(TSData[:, :n], axis=1) #market proxy TS: average over all\n","\n","  TSDatanp = TSData\n","  TSData = pd.DataFrame(TSData)\n","  #Calculate features\n","  Data = pd.DataFrame(np.zeros((n+1, num_feat)), columns = features)\n","  \n","  Data['std'] = TSData.std(axis=0)\n","  Data['skew'] = TSData.skew(axis=0)\n","  Data['kurt'] = TSData.kurtosis(axis=0)\n","  Data['VaR0.05'] = TSData.quantile(q=0.05, axis=0)\n","  Data['VaR0.95'] = TSData.quantile(q=0.95, axis=0)\n","  Data['ES'] = TSData[TSData  < Data['VaR0.05']].mean(axis=0)\n","  Data['EU'] = TSData[TSData  > Data['VaR0.95']].mean(axis=0)\n","  \n","  #CAPM beta\n","  r_market = TSDatanp[:,n].reshape(t,1)\n","  for j in range(n+1):\n","    X = TSDatanp[:,j].reshape(t,1)\n","    reg = LinearRegression().fit(X, r_market)\n","    Data.loc[j,'beta']=reg.coef_\n","    #Data.loc[j,'alpha']=reg.intercept_ #alpha only if necessary\n","\n","  # 'autocor', 'Hurst', 'stab_a', 'stab_g'\n","  for j in range(n+1):\n","    series = TSData.iloc[:,j]\n","    #autocorrelation coefficient\n","    Data.loc[j,'autocor'] = series.autocorr()\n","    #Hurst\n","    Data.loc[j, 'Hurst'] = hurst_rs(series) #using nolds package\n","    #fit levy stable distribution\n","    levystab = levy.fit_levy(series)\n","    #alpha stable\n","    Data.loc[j, 'stab_a'] = levystab[0].get()[0]\n","    #gamma stable\n","    Data.loc[j, 'stab_g'] = levystab[0].get()[3]\n","\n","  Data_save[i,:,:] = Data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GJAGvM3EMXV3"},"source":["# Compute PCA/Factor Analysis and calculate Adjusted Rand Index"]},{"cell_type":"code","metadata":{"id":"mg4YcfivMYZf"},"source":["for i in range(NumSim):\n","  Data = pd.DataFrame(Data_save_new[i,:,:], columns=features)\n","  #Scale the data\n","  X = StandardScaler().fit_transform(Data.loc[:n-1, :])\n","\n","  #Clustering\n","  km_model = KMeans(n_clusters=3)\n","\n","  #PCA (in this case necessary to use scaled data), then Clustering\n","  pca2 = PCA(2)\n","  pca3 = PCA(3)\n","  Y_2 = pca2.fit_transform(X)\n","  Y_3 = pca3.fit_transform(X)\n","\n","  Adj_Rand.loc[i, 'pca2_km'] = km(km_model, Y_2, y_true)\n","  Adj_Rand.loc[i, 'pca3_km'] = km(km_model, Y_3, y_true)\n","  Adj_Rand.loc[i, 'pca2_ac'] = agc(Y_2, y_true)\n","  Adj_Rand.loc[i, 'pca3_ac'] = agc(Y_3, y_true)\n","\n","  #Factor Analysis, then clustering\n","  fac2 = FactorAnalysis(n_components=2)#Factor Analysis with 2 factors\n","  fac3 = FactorAnalysis(n_components=3)#Factor Analysis with 3 factors\n","  fac2_vari = FactorAnalysis(n_components=2, rotation = 'varimax')#FA with 2 factors and Varimax rotation\n","  fac3_vari = FactorAnalysis(n_components=3, rotation = 'varimax')#FA with 3 factors and Varimax rotation\n","\n","  F_2 = fac2.fit_transform(X)\n","  F_3 = fac3.fit_transform(X)\n","  F_2V = fac2_vari.fit_transform(X)\n","  F_3V = fac3_vari.fit_transform(X)\n","  \n","  Adj_Rand.loc[i, 'fa2_km'] = km(km_model, F_2, y_true)\n","  Adj_Rand.loc[i, 'fa3_km'] = km(km_model, F_3, y_true)\n","  Adj_Rand.loc[i, 'fa2v_km'] = km(km_model, F_2V, y_true)\n","  Adj_Rand.loc[i, 'fa3v_km'] = km(km_model, F_3V, y_true)\n","  Adj_Rand.loc[i, 'fa2_ac'] = agc(F_2, y_true)\n","  Adj_Rand.loc[i, 'fa3_ac'] = agc(F_3, y_true)\n","  Adj_Rand.loc[i, 'fa2v_ac'] = agc(F_2V, y_true)\n","  Adj_Rand.loc[i, 'fa3v_ac'] = agc(F_3V, y_true)"],"execution_count":null,"outputs":[]}]}